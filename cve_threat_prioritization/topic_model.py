import nltk
import re
import os
from gensim import corpora
from gensim.models import LdaModel
lemmatizer = nltk.stem.WordNetLemmatizer()
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd

import config


class TopicModelParams:
    def __init__(self, num_topics, passes, iterations):
        self.num_topics = num_topics
        self.passes = passes
        self.iterations = iterations


TOPIC_MODEL_PARAMS = TopicModelParams(
    num_topics=config.NUM_TOPICS,
    passes=config.PASSES,
    iterations=config.ITERATIONS
)


def preprocess_text(df):
    """
    Preprocess the text data by removing non-alphabetic characters, stop words, URLs, and lemmatizing the text.
    """
    summaries = df[["cve_id", "summary"]]

    stopwords_ = nltk.corpus.stopwords.words("english")

    # Remove non-alphabetic characters
    summaries.loc[:, "summary"] = summaries["summary"].apply(
        lambda x: ' '.join([word for word in x.split() if word.isalpha()]))

    # Remove stop words from descriptions
    summaries.loc[:, "summary"] = summaries["summary"].apply(
        lambda x: ' '.join([word for word in x.split() if word.lower() not in (stopwords_)]))

    # Remove URLs from descriptions
    summaries.loc[:, "summary"] = summaries["summary"].apply(lambda x: re.sub(r'http\S+', '', x))

    # Lemmatize descriptions and format to lowercase
    summaries.loc[:, "summary"] = summaries["summary"].apply(
        lambda x: ' '.join([lemmatizer.lemmatize(word).lower() for word in x.split()]))

    return summaries


def get_dictionary_and_corpus(summaries):
    """
    Returns the dictionary and corpus for the LDA model.
    """
    processed_summaries = [doc.split() for doc in summaries["summary"]]
    dictionary = corpora.Dictionary(processed_summaries)
    corpus = [dictionary.doc2bow(doc) for doc in processed_summaries]

    return dictionary, corpus


def get_is_trained():
    if os.listdir("topic_model"):
        if "lda_model" in os.listdir("topic_model"):
            return True
    return False


def train_model(corpus, dictionary):
    print("Training LDA model...")
    lda_model = LdaModel(corpus,
                         num_topics=TOPIC_MODEL_PARAMS.num_topics, id2word=dictionary,
                         passes=TOPIC_MODEL_PARAMS.passes, iterations=TOPIC_MODEL_PARAMS.iterations)
    lda_model.save(os.path.join("topic_model", "lda_model"))

    return lda_model


def load_model():
    print("Loading LDA model...")
    lda_model = LdaModel.load(os.path.join("topic_model", "lda_model"))

    return lda_model


def plot_word_cloud(lda_model, num_topics, num_words=10):
    for i in range(num_topics):
        plt.figure(figsize=(10, 5))
        plt.title(f'Topic #{i + 1}')

        words = dict(lda_model.show_topic(i, num_words))

        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(words)

        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.show()
        plt.close()


def set_topics_probabilities(lda_model, corpus, summaries):
    topic_distributions = [lda_model.get_document_topics(bow) for bow in corpus]
    topic_df = pd.DataFrame([[dict(topics).get(i, 0) for i in range(TOPIC_MODEL_PARAMS.num_topics)] for topics in topic_distributions],
                            columns=[f"topic_{i+1}" for i in range(TOPIC_MODEL_PARAMS.num_topics)])

    topics_cve_df = pd.concat([summaries["cve_id"], topic_df], axis=1)

    return topics_cve_df



